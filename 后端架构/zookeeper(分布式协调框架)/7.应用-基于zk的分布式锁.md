#### 1. 背景

>在开发应用的时候，如果需要对某一个共享变量进行多线程同步访问的时候，可以使用Java多线程进行处理，并且可以完美的运行，毫无Bug！注意这是单机应用，也就是==所有的请求都会分配到当前服务器的JVM内部==，*<u>然后映射为操作系统的线程进行处理</u>*！==这个共享变量只是在这个JVM内部的一块内存空间==！
>
>后来业务发展，需要做集群，一个应用需要部署到几台机器上然后做负载均衡，大致如下图：
>
><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gmd5ls4fzsj30oa0bqwer.jpg" style="zoom:80%">
>
>- 成员变量A存在JVM1、JVM2、JVM3三个JVM内存中
>- 成员变量A同时都会在JVM分配一块内存，三个请求发过来同时对这个变量操作，显然结果是不对的
>- 不是同时发过来，三个请求分别操作三个不同JVM内存区域的数据，变量A之间不存在共享，也不具有可见性，处理的结果也是不对的
>
>注：该成员变量A是一个有状态的对象。
>
>当有==多个客户端==需要==访问并操作同一个资源==，还需要==保持这个资源一致性==的时候，就需要使用分布式锁，让多客户端==互斥==的对共享资源进行访问。

#### 2. 原理

>1）zookeeper里有互斥性，比如创建了一个lock节点
>
>```bash
>create /lock
>```
>
>再次创建lock会失败
>
>```bash
>create /lock
>```
>
>Node already exists: /lock

>1）同样可以给lock节点添加一个watcher：
>
>```bash
>get -w /lock
>```
>
>2）这样当其他人删除了这个lock节点便会收到通知：
>
>```bash
>delete /lock
>```
>
>```
>WATCHER::
>
>WatchedEvent state:SyncConnected type:NodeDeleted path:/lock
>```
>
>3）然后其他进程就可以去竞争这个资源

#### 3. zookeeper分布式锁的流程

><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gmd6aw5id4j317k0u0goh.jpg" style="zoom:60%">
>
>1. 获取锁，判断当前锁是否已经由其他事务创建。
>2. 如果已经创建，则监听/等待。`get -w /exclusive/lock`
>3. 如果没有创建，则创建。`create -e /exclusive/lock`
>4. 如果创建失败则监听/等待
>5. 如果创建成功，则获得锁，完成业务处理，释放锁。`delete /exclusive/lock`

#### 4. 缺陷

>那么问题来了？如果同时有1000个客户端发起请求并创建临时节点，都会去监听A结点的变化，然后A删除节点的时候会通知其他节点，这样是否会太影响并耗费资源了？那么怎么有效避免羊群效应呢？
>
>**羊群效应**：就是一个特定的==znode 改变的时候ZooKeper 触发了所有watches 的事件==。

#### 5. 优化方式

><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gmd6wf8em1j30jw0icgmu.jpg" style="zoom:60%">
>
>1. 请求进来，直接在`/lock`节点下创建一个==临时顺序节点==。
>2. 判断自己是不是lock节点下，最小的节点。
>   1. ==是最小的，获得锁==
>   2. ==不是。对前面的节点进行监听==
>3. 获得锁的请求，处理完释放锁，即delete节点，然后后继第一个节点将收到通知，重复第2步判断。

#### 6. 使用分布式锁解决超卖问题

