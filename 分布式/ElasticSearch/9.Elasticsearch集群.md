#### 发现和集群形成

>发现和集群形成过程负责
>
>1. 发现节点
>2. 选举主节点
>3. 形成集群
>4. 并在每次更改时发布集群状态。节点之间的所有通信都是使用[传输](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-transport.html)层完成的 。
>
>以下过程和设置是发现和集群形成的一部分：

##### **[发现](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-hosts-providers.html)**

>发现是在主机未知的情况下（例如，节点刚启动时或先前的主机发生故障时），==节点之间相互查找的过程==。

##### **[基于仲裁的决策](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-quorums.html)**

>即使某些节点不可用，Elasticsearch如何使用基于仲裁的投票机制进行决策。

##### **[投票配置](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-voting.html)**

>当节点离开并加入集群时，Elasticsearch如何自动更新投票配置。

##### **[引导集群](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-bootstrap-cluster.html)**

>当Elasticsearch集群首次启动时，需要引导集群。在[开发模式下](https://www.elastic.co/guide/en/elasticsearch/reference/current/bootstrap-checks.html#dev-vs-prod-mode)（未配置发现设置），这将由节点本身自动执行。由于这种自动引导在[本质上](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-quorums.html)是 [不安全的](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-quorums.html)，因此在[生产模式下](https://www.elastic.co/guide/en/elasticsearch/reference/current/bootstrap-checks.html#dev-vs-prod-mode)运行节点 需要通过[`cluster.initial_master_nodes` setting](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-bootstrap-cluster.html)显式配置引导 。

##### **[添加和删除符合主机资格的节点](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-adding-removing-nodes.html)**

>建议在群集中拥有少量固定数量的主资格节点，并仅通过添加和删除不符合主资格的节点来扩大和缩小群集。但是，在某些情况下，可能需要向集群添加或删除某些符合主机要求的节点。本节介绍添加或删除符合主机要求的节点的过程，包括同时删除一半以上符合主机要求的节点时需要执行的额外步骤。

##### **[发布集群状态](https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-state-publishing.html)**

>群集状态发布是选举的主节点通过其更新群集中所有其他节点上的群集状态的过程。

##### **[集群故障检测](https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-fault-detection.html)**

>Elasticsearch执行运行状况检查以检测和删除故障节点。

##### **[设定值](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-settings.html)**

>有一些设置可以使用户影响发现，集群形成，主选举和故障检测过程。



#### 1. 集群节点

>ES的集群是由==多个节点组成==的，通过`cluster.name`设置集群名称，并且用于区分其他的集群，每个节点通过`node.name`指定节点的名称。
>
>在ES中，节点的类型主要有4种：
>
>1. **master节点**
>   - 配置文件中node.master属性为true（默认为true），就有资格被选为master节点。
>   - ==master节点用于控制整个集群的操作。比如创建或删除索引，管理其他非master节点等==。
>
>2. **data节点**
>   - 配置文件中node.data属性为true（默认为true），就有资格被设置成data节点等。
>   - ==data节点主要用于执行数据相关的操作。比如文档的CRUD==。
>
>3. **客户端节点**
>   - 配置文件中`node.master`属性和`node.data`属性均为false。
>   - 该节点不能作为master节点，也不能作为data节点。
>   - 客户端节点==用于响应用户的请求，把请求转发到其他节点==。
>
>4. **部落节点**
>   - 当一个节点配置`tribe.*`的时候，它是一个特殊的客户端，它可以连接多个集群，在所有连接的集群上执行搜索和其他操作。

#### 2. 搭建集群

>```
># 启动3个虚拟机，分别在3台虚拟机上部署安装Elasticsearch
>mkdir /chance/es-cluster
>
># 分发到其它机器
>scp -r es-cluster elsearch@192.168.40.134:/chance
>
># node01的配置
>cluster.name: es-chance-cluster
>node.name: node01
>node.master: true
>node.data: true
>network.host: 0.0.0.0
>http.port: 9200
>discovery.zen.ping.unicast.hosts: ["192.168.40.133","192.168.40.134","192.168.40.135"]
>discovery.zen.minimum_master_nodes: 2
>http.cors.enabled: true
>http.cors.allow-origin: "*"
>
># node02的配置
>cluster.name: es-chance-cluster
>node.name: node02
>node.master: false
>node.data: true
>network.host: 0.0.0.0
>http.port: 9200
>discovery.zen.ping.unicast.hosts: ["192.168.40.133","192.168.40.134","192.168.40.135"]
>discovery.zen.minimum_master_nodes: 2
>http.cors.enabled: true
>http.cors.allow-origin: "*"
>
># node03的配置
>cluster.name: es-chance-cluster
>node.name: node03
>node.master: false
>node.data: true
>network.host: 0.0.0.0
>http.port: 9200
>discovery.zen.ping.unicast.hosts: ["192.168.40.133","192.168.40.134","192.168.40.135"]
>discovery.zen.minimum_master_nodes: 2
>http.cors.enabled: true
>http.cors.allow-origin: "*"
>```
>
>分别启动3个节点：
>
>```
>./elasticsearch
>```

>**查看集群状态：/_cluster/health**
>
>集群状态有三种颜色：
>
>| 颜色   | 意义                                       |
>| ------ | ------------------------------------------ |
>| green  | 所有主要分片和复制分片可用                 |
>| yellow | 所有主要分片可用，但不是所有复制分片都可用 |
>| red    | 不是所有的主要分片都可用                   |

#### 3. 分片和副本

>为了将数据添加到Elasticsearch，我们需要**索引(index)**——==一个存储关联数据的地方==。实际上，索引只是一个用来指向一个或多个分片的“逻辑命名空间(logical namespace)”。
>
>- 一个分片(shard)是一个最小级别“工作单元”，它只是==保存了索引中所有数据的一部分==。
>- ==分片就是一个Lucene实例==，并且它本身就是一个完整的搜索引擎。应用程序不会和它直接通信。
>- 分片可以是主分片(primary shard)或者复制分片(replica shard)。
>- 索引中的每个文档属于一个单独的主分片，所以主分片的数量决定了索引最多能存储多少数据。
>- 复制分片只是主分片的一个副本，它可以防止硬件故障导致的数据丢失，同时可以提供读请求，比如搜索或者从别的shard取回文档。
>- ==当索引创建完成的时候，主分片的数量就固定了，但是复制分片的数量可以随时调整==。

#### 4. 故障转移

##### 4.1 将data节点停止

>这里选择将node02停止。
>
>- 当集群状态为黄色，表示主节点可用，副本节点不完全可用。
>- 过一段时间观察，发现节点列表中看不到node02，副本节点分配到了node01和node03，集群状态恢复到绿色。

##### 4.2 将master节点停止

>接下来测试将node01停止，即主节点停止。
>
>集群对master进行了重新选举，选择node03为mater。并且集群状态变成黄色。等待一段时间后，集群状态从黄色变为绿色。
>
><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1glofb24am0j30cw0ew0sz.jpg" style="zoom:50%">
>
>恢复master节点，重启之后，发现node01可以正常加入到集群中，集群状态依然为绿色。

>说明：
>
>如果在配置文件中discovery.zen.minum_master_nodes设置的不是`N/2+1`时，会出现脑裂问题，之前宕机的主节点恢复后不会加入到集群。
>
><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1gloffssfzlj30uw0ec0tu.jpg" style="zoom:60%">

#### 5. 分布式文档

##### 5.1路由

><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1glpr071xndj30uk09k74w.jpg" style="zoom: 60%;">
>
>如图所示：向一个集群保存文档时，文档该存储到哪个节点呢？是随机的吗？是轮询吗？
>
>在ES中，会采用计算的方式来确定存储到哪个节点，计算公式如下：
>
>`shard = hash(routing) % number_of primary_shards`
>
>- `routing`值是一个任意字符串，默认是`_id`，也可以自定义。
>- routing字符串通过哈希函数生成一个数字，然后除以主切片的数量得到一个余数，余数的范围永远是0到number_of_primary_shards - 1，这个数字就是特定文档所在的分片。==这就是为什么创建了主分片后，不能修改的原因==。

##### 5.2 文档的写操作

>新建、索引和删除请求都是写操作，它们*<u>必须在主分片上成功完成后</u>*才能复制到相关的复制分片上。
>
><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1glpraw308hj30ty0dcgmt.jpg" style="zoom:60%;" />
>
>1. 客户端给node1发送新建、索引或删除请求。
>2. 节点使用文档的_id确定文档属于分片0。它转发请求到node3，分片0位于这个节点上。
>3. node3在主分片上执行请求，如果成功，它转发请求到相应的位于node1和node2的复制节点上。当所有的复制节点报告成功，node3报告成功到请求的节点，请求的节点再报告给客户端。

##### 5.3 搜索文档（单个文档）

>文档能够从主分片或任意一个复制分片被检索。
>
><img src="https://tva1.sinaimg.cn/large/0081Kckwgy1glprpng3voj30tu0b0js9.jpg" style="zoom:60%;">
>
>1. 客户端给node1发送get请求。
>2. 节点使用文档的_id确定文档属于分片0。分片0对应的复制分片在三个节点上都有。此时，它转发请求到node2。
>3. node返回文档(document)给node1然后返回给客户端。
>
>对于读请求，为了平衡负载，请求节点会为每个请求选择不同的分片——它会循环所有分片副本。
>
>可能的情况是，一个被索引的文档已经存在于主分片上却还没来得及同步到复制分片上。这时复制分片会报告文档未找到，主分片会成功返回文档。一旦索引请求成功返回给用户，文档则在主分片和复制分片都是可用的。

##### 5.4 全文搜索

>对于全文搜索而言，文档可能分散在各个节点上，那么在分布式的情况下，如何搜索文档呢？
>
>分为两个阶段：搜索`query` + 取回`fetch`

>`query:`
>
>1. 客户端发送一个search请求给node3，node3创建了一个长度为from+size的空优先级队列
>2. node3转发这个搜索请求到索引中每个分片的原本和副本。每个分片在本地执行这个查询并且将结果放到一个大小为from+size的有序本地优先队列里去
>3. 每个分片返回document的ID和它优先队列里的所有document的排序值给协调节点node3，node3把这些值合并到自己的优先队列里产生全局排序结果。
>
>`fetch`
>
>
>
>分发阶段由以下步骤构成：
>
>1. 协调节点辨别出哪个document需要取回，并且向相关分片发出GET请求。
>2. 每个分片加载document并且根据需要丰富它们，然后再将document返回协调节点。
>3. 一旦所有的document都被取回，协调节点会将结果返回给客户端。


