#### 1. 什么是分词

>`分词`就是指*<u>将一个文本转化成一系列单词的过程</u>*，也叫`文本分析`，在ES中称之为`Analysis`。
>
>如：我是中国人 --》 我/是/中国人

#### 2. 分词api

>指定分析器进行分词。
>
>```json
>POST /_analyze
>
>{
>    "analyzer": "standard", #指定标准分词器
>    "text": "hello world" #文本
>}
>```
>
>```json
>
>    "tokens": [
>        {
>            "token": "hello",
>            "start_offset": 0,
>            "end_offset": 5,
>            "type": "<ALPHANUM>",
>            "position": 0
>        },
>        {
>            "token": "world",
>            "start_offset": 6,
>            "end_offset": 11,
>            "type": "<ALPHANUM>",
>            "position": 1
>        }
>    ]
>}
>```
>
>结果中不仅可以看出分词的结果，还返回了该词在文本中的位置。

##### 2.1 指定索引分词

>```json
>POST /itcast/_analyze
>
>{
>    "analyzer": "standard",
>    "field": "hobby",
>    "text": "听音乐"
>}
>```

#### 3. 中文分词

>中文分词的难点在于，在汉语中没有明显的词汇分界点，如在英语中，空格可以作为分隔符，如果分隔不正确就会造成歧义。

>常用中文分词器：`IK`、`jieba`、`THULAC`等，==推荐使用IK分词器==。
>
>IK分词器：采用“正向迭代最细粒度切分算法”，具有80万字/秒的告诉处理能力，采用了多子处理器分析模式，支持：英文字母、数字、中文词汇等分词处理。优化的词典存储，更小的内存占用。

>
>
>```json
>POST /itcast/_analyze
>
>{
>    "analyzer": "ik_max_word",
>    "field": "hobby",
>    "text": "听音乐"
>}
>```
>
>```json
>{
>    "tokens": [
>        {
>            "token": "听音乐",
>            "start_offset": 0,
>            "end_offset": 3,
>            "type": "CN_WORD",
>            "position": 0
>        },
>        {
>            "token": "听音",
>            "start_offset": 0,
>            "end_offset": 2,
>            "type": "CN_WORD",
>            "position": 1
>        },
>        {
>            "token": "音乐",
>            "start_offset": 1,
>            "end_offset": 3,
>            "type": "CN_WORD",
>            "position": 2
>        }
>    ]
>}
>```
>
>

